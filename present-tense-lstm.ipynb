{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://gist.github.com/jovianlin/0a6b7c58cde7a502a68914ba001c77bf\n",
    "def load_glove_embeddings(fp, embedding_dim, include_empty_char=False):\n",
    "    \"\"\"\n",
    "    Loads pre-trained word embeddings (GloVe embeddings)\n",
    "        Inputs: - fp: filepath of pre-trained glove embeddings\n",
    "                - embedding_dim: dimension of each vector embedding\n",
    "                - include_empty_char: whether to include empty char in vocab\n",
    "        Outputs:\n",
    "                - word2coefs: Dictionary. Word to embedding vector\n",
    "                - word2index: Dictionary. Word to word-index\n",
    "                - embedding_matrix: Embedding matrix for Keras Embedding layer\n",
    "    \"\"\"\n",
    "    # First, build the \"word2coefs\" and \"word2index\"\n",
    "    word2coefs = {} # word to its corresponding coefficients\n",
    "    word2index = {} # word to word-index\n",
    "    with open(fp) as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            try:\n",
    "                data = [x.strip().lower() for x in line.split()]\n",
    "                word = data[0]\n",
    "                coefs = np.asarray(data[1:embedding_dim+1], dtype='float32')\n",
    "                word2coefs[word] = coefs\n",
    "                if word not in word2index:\n",
    "                    word2index[word] = len(word2index)\n",
    "            except Exception as e:\n",
    "                print('Exception occurred in `load_glove_embeddings`:', e)\n",
    "                continue\n",
    "        # End of for loop.\n",
    "    # End of with open\n",
    "    if include_empty_char:\n",
    "        word2index[''] = len(word2index)\n",
    "    # Second, build the \"embedding_matrix\"\n",
    "    # Words not found in embedding index will be all-zeros. Hence, the \"+1\".\n",
    "    vocab_size = len(word2coefs)+1 if include_empty_char else len(word2coefs)\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    for word, idx in word2index.items():\n",
    "        embedding_vec = word2coefs.get(word)\n",
    "        if embedding_vec is not None and embedding_vec.shape[0]==embedding_dim:\n",
    "            embedding_matrix[idx] = np.asarray(embedding_vec)\n",
    "    return word2coefs, word2index, embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48.68970071862351"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.14375**2 + 0.29442**2 + 0.078571**2 + 0.30209**2 + 0.47561**2 + 0.43339**2 + 0.11853**2 + 0.24294**2 + 0.15266**2 + 0.88948**2 + 0.63836**2 + 0.98421**2 + 0.2926**2 + 0.25954**2 + 0.28813**2 + 1.101**2 + 0.47256**2 + 0.11681**2 + 0.22856**2 + 0.43835**2 + 0.34791**2 + 0.82372**2 + 0.2971**2 + 0.75179**2 + 0.005743**2 + 0.36858**2 + 0.47466**2 + 0.19271**2 + 0.56166**2 + 0.35944**2 + 0.39476**2 + 0.26048**2 + 0.32528**2 + 1.1848**2 + 0.3129**2 + 0.39972**2 + 0.51549**2 + 0.17256**2 + 0.376**2 + 0.11885**2 + 0.31198**2 + 0.20403**2 + 0.38001**2  + 0.28433**2 + 0.41899**2 + 0.0038793**2 + 3.819**2 + 0.16336**2 + 0.05984**2 + 0.34421**2 + 0.13537**2 + 0.43697**2 + 1.2651**2 + 1.7427**2 + 0.22468**2 + 1.8752**2 + 0.4882**2 + 0.29383**2 + 0.92739**2 + 0.31286**2 + 0.93202**2 + 1.2429**2 + 0.46347**2 + 0.78895**2 + 1.0129**2  + 0.95664**2 + 0.40652**2 + 0.33332**2 + 0.30782**2 + 0.30168**2 + 0.55523**2 + 0.87218**2 + 0.38666**2 + 0.23546**2 + 0.35067**2 + 0.27966**2 + 0.83493**2 + 0.27571**2 + 0.88204**2 + 1.1066**2 + 0.24833**2 + 0.55462**2 + 0.31548**2 + 0.013784**2 + 1.2792**2 + 0.10665**2 + 0.18128**2 + 0.42517**2 + 0.18244**2 + 0.14501**2 + 0.38981**2 + 0.22133**2 + 0.048625**2 + 0.4338**2 + 0.56485**2 + 0.36333**2 + 0.079428**2 + 0.93321**2 + 0.31841**2 + 0.24426**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = ''\n",
    "GLOVE_FILE = os.path.join(BASE_DIR, 'glove.6B/glove.6B.100d.txt')\n",
    "TEXT_DATA_DIR = os.path.join(BASE_DIR, 'sents')\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "\n",
    "print('Indexing word vectors and preparing embedding matrix.')\n",
    "\n",
    "glove_word2coefs, glove_word2index, glove_embedding_matrix = load_glove_embeddings(GLOVE_FILE, EMBEDDING_DIM)\n",
    "\n",
    "print('Found %s word vectors.' % len(word2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second, prepare text samples\n",
    "print('Processing text dataset')\n",
    "\n",
    "texts = []  # list of sentences\n",
    "trans_texts = []  # list of transformed sentences\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "    path = os.path.join(TEXT_DATA_DIR, name)\n",
    "    if os.path.isdir(path):\n",
    "        for fname in sorted(os.listdir(path)):\n",
    "            if fname.isdigit():\n",
    "                fpath = os.path.join(path, fname)\n",
    "                #args = {} if sys.version_info < (3,) else {'encoding': 'latin-1'}\n",
    "                #with open(fpath, **args) as f:\n",
    "                with open(fpath, 'r') as f:\n",
    "                    s = ''\n",
    "                    t = ''\n",
    "                    while s is not None and t is not None:\n",
    "                        s = f.readline()\n",
    "                        if s:\n",
    "                            sent = s.strip()\n",
    "                            if sent and sent[0] == '<':\n",
    "                                sent = sent[2:]\n",
    "                                t = f.readline()\n",
    "                                if t:\n",
    "                                    t = t.strip()\n",
    "                                    sent_trans = t[2:]\n",
    "                                    texts.append(sent)\n",
    "                                    trans_texts.append(sent_trans)\n",
    "print('Found %s sentences.' % len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, lower=True, oov_token='UNK')\n",
    "tokenizer.fit_on_texts(texts + trans_texts)\n",
    "text_word2index = tokenizer.word_index  # Maps words to indices.\n",
    "\n",
    "in_sequences = tokenizer.texts_to_sequences(texts)\n",
    "out_sequences = tokenizer.texts_to_sequences(trans_texts)\n",
    "\n",
    "in_data = pad_sequences(in_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "out_data = pad_sequences(out_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "num_words = min(MAX_NUM_WORDS, len(text_word2index)) + 1\n",
    "\n",
    "print('Found %s unique tokens.' % len(text_word2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store embeddings of words in the text based on their indices\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in text_word2index.items():\n",
    "    if i > MAX_NUM_WORDS:\n",
    "        continue\n",
    "    if word in glove_word2index:\n",
    "        embedding_vector = glove_embedding_matrix[glove_word2index[word]]\n",
    "    else:\n",
    "        embedding_vector = np.random.normal(loc=0, scale=1, size=(1, EMBEDDING_DIM))\n",
    "        embedding_vector *= np.sqrt(32 / (np.sum(np.square(embedding_vector)))\n",
    "    embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained word embeddings into an Embedding layer\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "maxlen = 50\n",
    "\n",
    "# build the model: a single LSTM\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(len(chars), activation='softmax'))\n",
    "                                    \n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['acc'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          validation_data=(x_val, y_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
