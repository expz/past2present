{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import GRU\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://gist.github.com/jovianlin/0a6b7c58cde7a502a68914ba001c77bf\n",
    "def load_glove_embeddings(fp, embedding_dim, include_empty_char=False):\n",
    "    \"\"\"\n",
    "    Loads pre-trained word embeddings (GloVe embeddings)\n",
    "        Inputs: - fp: filepath of pre-trained glove embeddings\n",
    "                - embedding_dim: dimension of each vector embedding\n",
    "                - include_empty_char: whether to include empty char in vocab\n",
    "        Outputs:\n",
    "                - word2coefs: Dictionary. Word to embedding vector\n",
    "                - word2index: Dictionary. Word to word-index\n",
    "                - embedding_matrix: Embedding matrix for Keras Embedding layer\n",
    "    \"\"\"\n",
    "    # First, build the \"word2coefs\" and \"word2index\"\n",
    "    word2coefs = {} # word to its corresponding coefficients\n",
    "    word2index = {} # word to word-index\n",
    "    with open(fp, 'r') as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            try:\n",
    "                data = [x.strip().lower() for x in line.split()]\n",
    "                word = data[0]\n",
    "                coefs = np.asarray(data[1:embedding_dim+1], dtype='float32')\n",
    "                word2coefs[word] = coefs\n",
    "                if word not in word2index:\n",
    "                    word2index[word] = len(word2index)\n",
    "            except Exception as e:\n",
    "                print('Exception occurred in `load_glove_embeddings`:', e)\n",
    "                continue\n",
    "        # End of for loop.\n",
    "    # End of with open\n",
    "    if include_empty_char:\n",
    "        word2index[''] = len(word2index)\n",
    "    # Second, build the \"embedding_matrix\"\n",
    "    # Words not found in embedding index will be all-zeros. Hence, the \"+1\".\n",
    "    vocab_size = len(word2coefs)+1 if include_empty_char else len(word2coefs)\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    for word, idx in word2index.items():\n",
    "        embedding_vec = word2coefs.get(word)\n",
    "        if embedding_vec is not None and embedding_vec.shape[0]==embedding_dim:\n",
    "            embedding_matrix[idx] = np.asarray(embedding_vec)\n",
    "    return word2coefs, word2index, embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '/mnt'\n",
    "GLOVE_FILE = os.path.join(BASE_DIR, 'glove.6B/glove.6B.100d.txt')\n",
    "TEXT_DATA_DIR = os.path.join(BASE_DIR, 'sents')\n",
    "MAX_SEQUENCE_LENGTH = 70\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "TEST_SPLIT = 0.2\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors and preparing embedding matrix.\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "\n",
    "print('Indexing word vectors and preparing embedding matrix.')\n",
    "\n",
    "glove_word2coefs, glove_word2index, glove_embedding_matrix = load_glove_embeddings(GLOVE_FILE, EMBEDDING_DIM)\n",
    "\n",
    "print('Found %s word vectors.' % len(glove_word2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second, prepare text samples\n",
    "print('Processing text dataset')\n",
    "\n",
    "texts = []  # list of sentences\n",
    "trans_texts = []  # list of transformed sentences\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "    path = os.path.join(TEXT_DATA_DIR, name)\n",
    "    if os.path.isdir(path):\n",
    "        for fname in sorted(os.listdir(path)):\n",
    "            if fname.isdigit():\n",
    "                fpath = os.path.join(path, fname)\n",
    "                #args = {} if sys.version_info < (3,) else {'encoding': 'latin-1'}\n",
    "                #with open(fpath, **args) as f:\n",
    "                with open(fpath, 'r') as f:\n",
    "                    s = ''\n",
    "                    t = ''\n",
    "                    while s is not None and t is not None:\n",
    "                        s = f.readline()\n",
    "                        if s:\n",
    "                            sent = s.strip()\n",
    "                            if sent and sent[0] == '<':\n",
    "                                sent = sent[2:]\n",
    "                                t = f.readline()\n",
    "                                if t:\n",
    "                                    t = t.strip()\n",
    "                                    sent_trans = t[2:]\n",
    "                                    texts.append(sent)\n",
    "                                    trans_texts.append(sent_trans)\n",
    "print('Found %s sentences.' % len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, lower=True, oov_token='UNK')\n",
    "tokenizer.fit_on_texts(texts + trans_texts)\n",
    "text_word2index = tokenizer.word_index  # Maps words to indices.\n",
    "\n",
    "in_sequences = tokenizer.texts_to_sequences(texts)\n",
    "out_sequences = tokenizer.texts_to_sequences(trans_texts)\n",
    "\n",
    "in_data = pad_sequences(in_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "out_data = pad_sequences(out_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "num_words = min(MAX_NUM_WORDS, len(text_word2index)) + 1\n",
    "\n",
    "print('Found %s unique tokens.' % len(text_word2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store embeddings of words in the text based on their indices\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in text_word2index.items():\n",
    "    if i > MAX_NUM_WORDS:\n",
    "        continue\n",
    "    if word in glove_word2index:\n",
    "        embedding_vector = glove_embedding_matrix[glove_word2index[word]]\n",
    "    else:\n",
    "        embedding_vector = np.random.normal(loc=0, scale=1, size=(1, EMBEDDING_DIM))\n",
    "        embedding_vector *= np.sqrt(32 / (np.sum(np.square(embedding_vector)))\n",
    "    embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(in_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "in_data = in_data[indices]\n",
    "out_data = out_data[indices]\n",
    "num_test_samples = int(TEST_SPLIT * in_data.shape[0])\n",
    "num_validation_samples = int(VALIDATION_SPLIT * in_data.shape[0])\n",
    "\n",
    "x_train = in_data[:-num_validation_samples - num_test_samples]\n",
    "y_train = out_data[:-num_validation_samples - num_test_samples]\n",
    "x_test = in_data[-num_validation_samples - num_test_samples:-num_validation_samples]\n",
    "y_test = out_data[-num_validation_samples - num_test_samples:-num_validation_samples]\n",
    "x_val = in_data[-num_validation_samples:]\n",
    "y_val = out_data[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained word embeddings into an Embedding layer\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)\n",
    "maxlen = 50\n",
    "\n",
    "# build the model: a single LSTM\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(GRU(128, input_shape=(MAX_SEQUENCE_LENGTH, EMBEDDING_DIM)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(EMBEDDING_DIM, activation='softmax'))\n",
    "                                    \n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['acc'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          validation_data=(x_val, y_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
