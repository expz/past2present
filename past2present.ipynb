{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from pattern.en import conjugate\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Verb(object):\n",
    "    \"\"\"\n",
    "    This class if for those verbs which have the following dependency labels:\n",
    "\n",
    "    VERB ROOT\n",
    "    WHNP > WDT --relcl--> NN\n",
    "    VERB --advcl--> NN\n",
    "    VERB --advcl--> IN\n",
    "    VERB --ccomp--> NN\n",
    "    \"\"\"\n",
    "    AUXILIARY_MODALS = [\n",
    "        'can', 'could', 'may', 'might', 'must', 'shall', 'should', 'will',\n",
    "        'would'\n",
    "    ]\n",
    "\n",
    "    def __init__(self, tok=None, clause=None, number_person='pl'):\n",
    "        \"\"\"\n",
    "        If the verb comes from a fragment without a subject,\n",
    "        then default conjugation is past plural.\n",
    "        \"\"\"\n",
    "        self.is_parsed = False\n",
    "        self.break_recursion = False\n",
    "        self.tok = tok\n",
    "        self.clause = clause\n",
    "        self.aux = []\n",
    "        self.nsubj = None\n",
    "        self.do = False\n",
    "        self.been = False\n",
    "        self.number_person = number_person\n",
    "        self.is_past = False\n",
    "        self.is_future = False\n",
    "        self.have_pres = False\n",
    "        self.have_past = False\n",
    "        self.is_modal = False\n",
    "        self.is_participle = False\n",
    "        if self.tok and self.clause:\n",
    "            self.parse()\n",
    "\n",
    "    def __str__(self):\n",
    "        s = \"\"\"\n",
    "        is parsed: %s\n",
    "        verb: %s\n",
    "        verb lemma: %s\n",
    "        verb dependency label: %s\n",
    "        verb constituent tag: %s\"\"\" % (\n",
    "            self.is_parsed, self.tok.lower_,\n",
    "            self.tok.lemma_, self.tok.dep_, self.tok.tag_)\n",
    "        for i, a in enumerate(self.aux):\n",
    "            s += \"\"\"\n",
    "        aux %d: %s\n",
    "        aux %d lemma: %s\n",
    "        aux %d dependency label: %s\"\"\" % (\n",
    "                i, a.lower_, i, a.lemma_, i, a.dep_)\n",
    "        s += \"\"\"\n",
    "        subject: %s\n",
    "        number_person: %s\n",
    "        has 'do': %s\n",
    "        has 'been': %s\n",
    "        is past tense: %s\n",
    "        is future tense: %s\n",
    "        is participle: %s\n",
    "        has 'have' in present tense: %s\n",
    "        has 'have' in past tense: %s\n",
    "        has modal: %s\n",
    "\"\"\" % (\n",
    "            self.nsubj, self.number_person, self.do, self.been,\n",
    "            self.is_past, self.is_future, self.is_participle,\n",
    "            self.have_pres, self.have_past, self.is_modal)\n",
    "        return s\n",
    "\n",
    "    def parse(self):\n",
    "        if not self.tok or not self.clause:\n",
    "            raise Exception('Verb.parse_verb() requires the `tok` and `clause` '\n",
    "                            'variables to be set.')\n",
    "        if self.break_recursion:\n",
    "            raise Exception(\n",
    "                'There is a circular dependency among conjunctive verbs.')\n",
    "        # Reset variables.\n",
    "        self.aux = []\n",
    "        self.do = False\n",
    "        self.been = False\n",
    "        self.have_pres = False\n",
    "        self.have_past = False\n",
    "        self.is_modal = False\n",
    "        self.is_past = False\n",
    "        self.is_future = False\n",
    "        # Check for previous verb joined by conjunction.\n",
    "        self.break_recursion = True\n",
    "        prev_verb = self.clause.prev_verb(self)\n",
    "        self.break_recursion = False\n",
    "        if prev_verb:\n",
    "            self.nsubj = prev_verb.nsubj\n",
    "            self.number_person = prev_verb.number_person\n",
    "            for child in self.tok.children:\n",
    "                if child.lemma_ == 'not':\n",
    "                    self.not_token = child\n",
    "                elif child.dep_ == 'aux' or child.dep_ == 'auxpass':\n",
    "                    self.aux.append(child)\n",
    "            if not self.aux:\n",
    "                if prev_verb.do:\n",
    "                    # Special exception: He did not walk but talked.\n",
    "                    if self.tok.tag_ == 'VBD':\n",
    "                        self.do = False\n",
    "                        self.aux = []\n",
    "                    else:\n",
    "                        self.do = True\n",
    "                        self.aux = prev_verb.aux\n",
    "                else:\n",
    "                    self.do = False\n",
    "                    self.aux = prev_verb.aux\n",
    "                self.been = prev_verb.been\n",
    "                self.have_pres = prev_verb.have_pres\n",
    "                self.have_past = prev_verb.have_past\n",
    "                self.is_modal = prev_verb.is_modal\n",
    "                self.is_past = prev_verb.is_past\n",
    "                self.is_future = prev_verb.is_future\n",
    "            else:\n",
    "                self.is_past = self._is_past()\n",
    "            # Calculate negation status.\n",
    "            # Algo: calculate negation status of previous verb\n",
    "            #       then calculate neg of current verb by\n",
    "            #       searching for not/n't/but\n",
    "            #\n",
    "            # * \"not\"/\"n't\" have dep_ == 'neg'\n",
    "            # * \"but\" has dep_ == 'cc' and is child of\n",
    "            #   the first verb of conjunction\n",
    "            #\n",
    "            # Ex:\n",
    "            #   did not talk or walk\n",
    "            #   did not talk but walked and balked\n",
    "            #   did talk but didn't walk or balk\n",
    "            #   didn't talk but did walk and balk\n",
    "        else:\n",
    "            # Iterates children in order of appearance.\n",
    "            for child in self.tok.children:\n",
    "                if child.dep_ == 'nsubj' or child.dep_ == 'nsubjpass':\n",
    "                    self.nsubj = child\n",
    "                elif child.dep_ == 'aux' or child.dep_ == 'auxpass':\n",
    "                    self.aux.append(child)\n",
    "            for a in self.aux:\n",
    "                if a.lemma_ == 'do':\n",
    "                    self.do = True\n",
    "                elif a.lower_ == 'been':\n",
    "                    self.been = True\n",
    "                elif a.lower_ == 'will':\n",
    "                    self.is_future = True\n",
    "                if (a.lower_ == 'have' or a.lower_ == 'has'):\n",
    "                    self.have_pres = True\n",
    "                if a.lower_ == 'had':\n",
    "                    self.have_past = True\n",
    "            self.is_modal = self.aux and self._is_aux_modal(self.aux[0])\n",
    "            self.number_person = self._number_person(self.nsubj)\n",
    "            self.is_past = self._is_past()\n",
    "        self.is_participle = self._is_participle(self.tok)\n",
    "        self.is_parsed = True\n",
    "\n",
    "    def transform_to_present_str(self, tok):\n",
    "        \"\"\"\n",
    "        Takes in a token. If the token is this verb or one of its auxiliaries,\n",
    "        then it converts it to present tense.\n",
    "\n",
    "        Assumes that `self` is a past tense verb.\n",
    "\n",
    "        Rules:\n",
    "        * past > present\n",
    "        * had + ppart > has/have + ppart\n",
    "        * had + been + ppart > has/have + been + ppart\n",
    "        * was + ppart > is + ppart\n",
    "        * did > does, had > has/have, was > is\n",
    "        * would have + ppart > would + inf\n",
    "        * would have + been + ppart> would be + ppart\n",
    "        \"\"\"\n",
    "        if self.aux and tok == self.aux[0]:\n",
    "            if tok.lemma_ == 'do':\n",
    "                return conjugate('do', self.number_person) + tok.whitespace_\n",
    "            elif self.is_modal:\n",
    "                return tok.text_with_ws\n",
    "            elif self.is_participle and tok.lower_ == 'has':\n",
    "                return conjugate('have', self.number_person) + tok.whitespace_\n",
    "            else:\n",
    "                return conjugate(tok.lemma_, self.number_person) \\\n",
    "                        + tok.whitespace_\n",
    "        elif tok in self.aux:\n",
    "            if self.is_modal and not self.is_future and tok.lower_ == 'have':\n",
    "                return ''\n",
    "            elif self.is_modal and not self.is_future and tok.lower_ == 'been':\n",
    "                return 'be' + tok.whitespace_\n",
    "            else:\n",
    "                return tok.text_with_ws\n",
    "        elif tok == self.tok:\n",
    "            if not self.aux:\n",
    "                return conjugate(tok.lemma_, self.number_person) \\\n",
    "                        + tok.whitespace_\n",
    "            elif self.is_modal and not self.been and not self.is_future:\n",
    "                return tok.lemma_ + tok.whitespace_\n",
    "            else:\n",
    "                return tok.text_with_ws\n",
    "        return None\n",
    "\n",
    "    @classmethod\n",
    "    def _is_aux_modal(cls, tok):\n",
    "        return tok.lower_ in cls.AUXILIARY_MODALS\n",
    "\n",
    "    @classmethod\n",
    "    def _contains_one_of(cls, haystack, needles):\n",
    "        for item in haystack:\n",
    "            for needle in needles:\n",
    "                if item == needle:\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    @classmethod\n",
    "    def _number_person(cls, token):\n",
    "        \"\"\"\n",
    "        Expects a token with POS == 'NOUN'.\n",
    "        \"\"\"\n",
    "        # Default is plural.\n",
    "        if not token:\n",
    "            return 'pl'\n",
    "        for tok in token.children:\n",
    "            if tok.lower_ == 'and':\n",
    "                return 'pl'\n",
    "        if (token.tag_ == 'NNPS' or token.tag_ == 'NNS' or\n",
    "                token.lower_ == 'we' or token.lower_ == 'they'):\n",
    "            return 'pl'\n",
    "        elif token.lower_ == 'i':\n",
    "            return '1sg'\n",
    "        elif token.lower_ == 'you':\n",
    "            return '2sg'\n",
    "        else:\n",
    "            return '3sg'\n",
    "\n",
    "    def _is_past(self):\n",
    "        if self.is_future:\n",
    "            return False\n",
    "        if self.do:\n",
    "            return self._is_past_tok(self.aux[0], self.number_person)\n",
    "        else:\n",
    "            if self.aux and self._is_past_tok(self.aux[0], self.number_person):\n",
    "                return True\n",
    "            elif self._is_past_tok(self.tok, self.number_person):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    @classmethod\n",
    "    def _is_past_tok(cls, tok, number_person=None):\n",
    "        \"\"\"\n",
    "        Is `verb` past tense, where `verb` has lemma `lemma`\n",
    "        and number/person `number_person`?\n",
    "        \"\"\"\n",
    "        PAST_CONJ = {\n",
    "            '1sg': '1sgp',\n",
    "            '2sg': '2sgp',\n",
    "            '3sg': '3sgp',\n",
    "            'pl': 'ppl',\n",
    "            'part': 'ppart',\n",
    "        }\n",
    "        if tok.tag_ == 'VBN' or tok.tag_ == 'VBD':\n",
    "            return True\n",
    "        if number_person:\n",
    "            return conjugate(tok.lemma_, PAST_CONJ[number_person]) == tok.text\n",
    "        return (conjugate(tok.lemma_, '1sgp') == tok.text or\n",
    "                conjugate(tok.lemma_, '2sgp') == tok.text or\n",
    "                conjugate(tok.lemma_, '3sgp') == tok.text)\n",
    "\n",
    "    @classmethod\n",
    "    def _is_participle(cls, tok):\n",
    "        \"\"\"\n",
    "        Expects a token.\n",
    "        \"\"\"\n",
    "        return tok.tag_ == 'VBG' or tok.tag_ == 'VBN'\n",
    "\n",
    "\n",
    "class Clause(object):\n",
    "    VERB_DEPS = ['ROOT', 'conj', 'relcl', 'advcl', 'ccomp']\n",
    "\n",
    "    def __init__(self, span):\n",
    "        self.verbs = []\n",
    "        self.span = span\n",
    "\n",
    "    def prev_verb(self, verb):\n",
    "        \"\"\"\n",
    "        If a verb is connected by a conjunction to a previous verb,\n",
    "        then get that verb.\n",
    "        \"\"\"\n",
    "        prev_verb = None\n",
    "        if verb.tok.dep_ == 'conj':\n",
    "            for tok in verb.tok.ancestors:\n",
    "                if tok.dep_ in self.VERB_DEPS:\n",
    "                    for v in self.verbs:\n",
    "                        if v.tok == tok:\n",
    "                            prev_verb = v\n",
    "                            if not prev_verb.is_parsed:\n",
    "                                prev_verb.parse()\n",
    "                    break\n",
    "        return prev_verb\n",
    "\n",
    "    def parse_verbs(self):\n",
    "        self.verbs = []\n",
    "        first = True\n",
    "        # Depth-first search to find verbs.\n",
    "        s = []\n",
    "        s.append(self.span)\n",
    "        while s:\n",
    "            span = s.pop()\n",
    "            if 'S' in span._.labels or 'SBAR' in span._.labels:\n",
    "                if first:\n",
    "                    first = False\n",
    "                else:\n",
    "                    continue\n",
    "            elif len(span) == 1 and span[0].dep_ in self.VERB_DEPS:\n",
    "                self.verbs.append(Verb(span[0], self))\n",
    "            s.extend(reversed(list(span._.children)))\n",
    "\n",
    "    def transform_present(self):\n",
    "        self.parse_verbs()\n",
    "        past_verbs = [v for v in self.verbs if v.is_past]\n",
    "        first = True\n",
    "        new_text = ''\n",
    "        # Depth-first search to transform past to present.\n",
    "        s = []\n",
    "        s.append(self.span)\n",
    "        while s:\n",
    "            span = s.pop()\n",
    "            if len(span) == 1:\n",
    "                span_is_past_verb = False\n",
    "                if span[0].pos_ == 'VERB':\n",
    "                    for v in past_verbs:\n",
    "                        present = v.transform_to_present_str(span[0])\n",
    "                        if present is not None:\n",
    "                            new_text += present\n",
    "                            span_is_past_verb = True\n",
    "                            break\n",
    "                if not span_is_past_verb:\n",
    "                    new_text += span.text_with_ws\n",
    "            elif 'S' in span._.labels or 'SBAR' in span._.labels:\n",
    "                if first:\n",
    "                    first = False\n",
    "                    s.extend(reversed(list(span._.children)))\n",
    "                else:\n",
    "                    ws = ' ' if span.text_with_ws[-1] == ' ' else ''\n",
    "                    new_text += Clause(span).transform_present() + ws\n",
    "            else:\n",
    "                s.extend(reversed(list(span._.children)))\n",
    "        new_text = new_text.replace(' .', '.').replace(' ,', ',')\n",
    "        if new_text and new_text[-1] == ' ':\n",
    "            new_text = new_text[:-1]\n",
    "        return new_text\n",
    "\n",
    "\n",
    "class Sentence(object):\n",
    "\n",
    "    def __init__(self, span):\n",
    "        self.span = span\n",
    "\n",
    "    def transform_present(self):\n",
    "        if not self.span:\n",
    "            raise Exception(\n",
    "                'Sentence.transform_present() requires `span` to be set.')\n",
    "        try:\n",
    "            text = Clause(self.span).transform_present()\n",
    "            if text:\n",
    "                text = text[0].upper() + text[1:]\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            print('There was an error parsing the following sentence')\n",
    "            print()\n",
    "            print(self.span.text)\n",
    "            print()\n",
    "            print('with parse:')\n",
    "            print()\n",
    "            if hasattr(self.span._, 'parse_string'):\n",
    "                print(self.span._.parse_string)\n",
    "            print()\n",
    "            print(str(e) + '\\n' + traceback.format_exc())\n",
    "            return self.span.text\n",
    "\n",
    "\n",
    "def transform_present(nlp, text, echo=False):\n",
    "    \"\"\"\n",
    "    It is not allowed to have sentences that conjunct verbs of different tenses\n",
    "    without auxiliary verbs:\n",
    "\n",
    "    * \"We will go tomorrow and go today.\"\n",
    "\n",
    "    This is because they are ambiguous without knowledge of adverbs like\n",
    "    \"tomorrow\" and \"today\":\n",
    "\n",
    "    \"We will go and eat.\"\n",
    "\n",
    "    Should eat be present or future tense? We assume future to avoid having to\n",
    "    analyze the semantics of adverbial phrases.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    trans_text = []\n",
    "    for sent in doc.sents:\n",
    "        trans_text.append(Sentence(sent).transform_present())\n",
    "        if echo:\n",
    "            print(trans_text[-1])\n",
    "    return ' '.join(trans_text)\n",
    "\n",
    "\n",
    "def transform_present_span(sent):\n",
    "    return Sentence(sent).transform_present()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import benepar\n",
    "from benepar.spacy_plugin import BeneparComponent\n",
    "from collections import namedtuple\n",
    "from past2present import Sentence\n",
    "import re\n",
    "import spacy\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeneparExtension(object):\n",
    "    \"\"\"\n",
    "    Represents a custom extension of a spacy Span with data from the\n",
    "    `benepar` plugin. Allows access of properties using dot syntax.\n",
    "    \"\"\"\n",
    "    def __init__(self, children=[], labels=[], parse_string=''):\n",
    "        self.children = children\n",
    "        self.labels = labels\n",
    "        self.parse_string = parse_string\n",
    "\n",
    "# BeneparExts = namedtuple('Extensions', ['children', 'labels', 'parse_string'])\n",
    "\n",
    "\n",
    "class SpacyToken(object):\n",
    "    \"\"\"\n",
    "    A pickleable version of a `spacy.tokens.token.Token` object. It holds a\n",
    "    pointer to the `SpacySentenceSpan` object that represents the sentence\n",
    "    in which it appears.\n",
    "    \"\"\"\n",
    "    def __init__(self, sent, tok):\n",
    "        self.sent = sent\n",
    "        self.dep_ = tok.dep_\n",
    "        self.i = tok.i\n",
    "        self.lemma_ = tok.lemma_\n",
    "        self.lower_ = tok.lower_\n",
    "        self.pos_ = tok.pos_\n",
    "        self.tag_ = tok.tag_\n",
    "        self.text = tok.text\n",
    "        self.text_with_ws = tok.text_with_ws\n",
    "        self.whitespace_ = tok.whitespace_\n",
    "        self.__children = [child.i for child in tok.children]\n",
    "        self.__ancestors = [anc.i for anc in tok.ancestors]\n",
    "\n",
    "    @property\n",
    "    def children(self):\n",
    "        for i in self.__children:\n",
    "            yield self.sent[i - self.sent.start]\n",
    "\n",
    "    @property\n",
    "    def ancestors(self):\n",
    "        for i in self.__ancestors:\n",
    "            yield self.sent[i - self.sent.start]\n",
    "\n",
    "\n",
    "class SpacySpan(object):\n",
    "    \"\"\"\n",
    "    A pickleable version of a `spacy.tokens.span.Span` object. It requires\n",
    "    the Berkeley Neural Parser `benepar` plugin, because it follows the\n",
    "    constituency parse tree to transform each `spacy.tokens.span.Span` into\n",
    "    a `SpacySpan`.\n",
    "    \"\"\"\n",
    "    def __init__(self, sent, span=None):\n",
    "        if span is None:\n",
    "            self.sent = sent\n",
    "            self.__len = 0\n",
    "            self.text = ''\n",
    "            self.text_with_ws = ''\n",
    "            self.start = 0\n",
    "            self.end = 0\n",
    "            self._ = BeneparExtension(children=[], labels=[], parse_string='')\n",
    "        else:\n",
    "            chs = [SpacySpan(sent, child) for child in span._.children]\n",
    "            ls = list(span._.labels)\n",
    "            ps = span._.parse_string\n",
    "            self.__len = len(span)\n",
    "            self.sent = sent\n",
    "            self.text = span.text\n",
    "            self.text_with_ws = span.text_with_ws\n",
    "            self.start = span.start\n",
    "            self.end = span.end\n",
    "            self._ = BeneparExtension(children=chs, labels=ls, parse_string=ps)\n",
    "\n",
    "    def _relative_index(self, idx):\n",
    "        if idx < 0:\n",
    "            idx += self.__len\n",
    "        idx += self.start - self.sent.start\n",
    "        return idx\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if isinstance(key, slice):\n",
    "            start = self.relative_index(slice.start)\n",
    "            stop = self.relative_index(slice.stop)\n",
    "            return self.sent[start:stop:slice.step]\n",
    "        elif isinstance(key, int):\n",
    "            return self.sent[self._relative_index(key)]\n",
    "        else:\n",
    "            raise ValueError('Index must be an integer or slice.')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.end - self.start\n",
    "\n",
    "\n",
    "class SpacySentenceSpan(SpacySpan):\n",
    "    \"\"\"\n",
    "    A pickleable version of a `spacy.tokens.span.Span` object that represents\n",
    "    an entire sentence.\n",
    "\n",
    "    A special type of span is required for this, because it needs to\n",
    "    hold the tokens that other spans refer to.\n",
    "    \"\"\"\n",
    "    def __init__(self, span):\n",
    "        self.__tokens = [SpacyToken(self, tok) for tok in span]\n",
    "        super(SpacySentenceSpan, self).__init__(self, span)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if isinstance(key, slice):\n",
    "            if slice.step != 1:\n",
    "                raise ValueError(\n",
    "                        'Stepped slices are not supported in Span objects.')\n",
    "            span = SpacySpan(self)\n",
    "\n",
    "            start = self.relative_index(slice.start)\n",
    "            stop = self.relative_index(slice.stop)\n",
    "\n",
    "            if len(self.__tokens[start:stop]) == 0:\n",
    "                return span\n",
    "            print('start:', start, ' stop:', stop)\n",
    "            all_but_last = map(lambda tok: tok.text_with_ws,\n",
    "                               self.__tokens[start:stop - 1])\n",
    "            last = self.__tokens[stop - 1]\n",
    "            span.text = ''.join(all_but_last) + last.text\n",
    "            span.text_with_ws = span.text + last.whitespace_\n",
    "            return span\n",
    "        elif isinstance(key, int):\n",
    "            return self.sent.__tokens[self._relative_index(key)]\n",
    "        else:\n",
    "            raise ValueError('Index must be an integer or slice.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_parse(nlp):\n",
    "    doc = nlp(u'The cat sat on the mat by a hat.')\n",
    "    sent = next(doc.sents)\n",
    "    psent = SpacySentenceSpan(sent)\n",
    "    for child in psent._.children:\n",
    "        assert isinstance(child, SpacySpan)\n",
    "    s = [(sent, psent)]\n",
    "    while s:\n",
    "        span, pspan = s.pop()\n",
    "        print('<', span.text)\n",
    "        print('>', pspan.text)\n",
    "        assert isinstance(pspan, SpacySpan)\n",
    "        assert len(span) == len(pspan)\n",
    "        assert span.text_with_ws == pspan.text_with_ws\n",
    "        assert span.text == pspan.text\n",
    "        if len(span) == 1:\n",
    "            assert isinstance(pspan[0], SpacyToken)\n",
    "            assert span[0].lemma_ == pspan[0].lemma_\n",
    "            assert span[0].dep_ == pspan[0].dep_\n",
    "            assert span[0].whitespace_ == pspan[0].whitespace_\n",
    "        s.extend(reversed(list(zip(span._.children, pspan._.children))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package benepar_en to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package benepar_en is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import benepar\n",
    "benepar.download('benepar_en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.add_pipe(BeneparComponent('benepar_en'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cat sits on the mat by a hat.\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'The cat sat on the mat by a hat.')\n",
    "sent = next(doc.sents)\n",
    "psent = SpacySentenceSpan(sent)\n",
    "print(transform_present_span(psent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< The cat sat on the mat by a hat.\n",
      "> The cat sat on the mat by a hat.\n",
      "< The cat\n",
      "> The cat\n",
      "< The\n",
      "> The\n",
      "< cat\n",
      "> cat\n",
      "< sat on the mat by a hat\n",
      "> sat on the mat by a hat\n",
      "< sat\n",
      "> sat\n",
      "< on the mat\n",
      "> on the mat\n",
      "< on\n",
      "> on\n",
      "< the mat\n",
      "> the mat\n",
      "< the\n",
      "> the\n",
      "< mat\n",
      "> mat\n",
      "< by a hat\n",
      "> by a hat\n",
      "< by\n",
      "> by\n",
      "< a hat\n",
      "> a hat\n",
      "< a\n",
      "> a\n",
      "< hat\n",
      "> hat\n",
      "< .\n",
      "> .\n"
     ]
    }
   ],
   "source": [
    "test_parse(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "\n",
    "from past.builtins import unicode\n",
    "\n",
    "import apache_beam as beam\n",
    "from apache_beam import coders\n",
    "from apache_beam.io import filebasedsource\n",
    "from apache_beam.io import WriteToText\n",
    "from apache_beam.io.iobase import Read\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from apache_beam.options.pipeline_options import SetupOptions\n",
    "from apache_beam.transforms import PTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParseWithSpacy(beam.DoFn):\n",
    "    DEFAULT_BATCH_SIZE = 16\n",
    "    DEFAULT_N_THREADS = 2\n",
    "\n",
    "    def process(self, doc_string, *args, **kwargs):\n",
    "        batch_size = (kwargs['batch_size'] if 'batch_size' in kwargs\n",
    "                      else self.DEFAULT_BATCH_SIZE)\n",
    "        n_threads = (kwargs['n_threads'] if 'n_threads' in kwargs\n",
    "                     else self.DEFAULT_N_THREADS)\n",
    "        doc_strings = [doc_string]\n",
    "        docs = self.nlp.pipe(doc_strings,\n",
    "                             batch_size=batch_size,\n",
    "                             n_threads=n_threads)\n",
    "        benepar = self.nlp.get_pipe('benepar')\n",
    "        docs = map(benepar, docs)\n",
    "        for doc in docs:\n",
    "            for sent in doc.sents:\n",
    "                yield SpacySentenceSpan(sent)\n",
    "\n",
    "    def start_bundle(self):\n",
    "        if not getattr(self, 'nlp', None):\n",
    "            self.nlp = ParseWithSpacy.init_spacy_english_model()\n",
    "        else:\n",
    "            logging.debug('Spacy model already initialized.')\n",
    "\n",
    "    @staticmethod\n",
    "    def init_spacy_english_model():\n",
    "        nlp = spacy.load('en_core_web_lg')\n",
    "        nlp.add_pipe(BeneparComponent('benepar_en'))\n",
    "        return nlp\n",
    "\n",
    "\n",
    "class _GutenbergSource(filebasedsource.FileBasedSource):\n",
    "    def __init__(self,\n",
    "                 file_pattern,\n",
    "                 chunk_char_length=4000,\n",
    "                 min_sentence_char_length=5):\n",
    "        super(_GutenbergSource, self).__init__(file_pattern, splittable=False)\n",
    "        self.chunk_char_length = chunk_char_length\n",
    "        self.min_sentence_char_length = min_sentence_char_length\n",
    "        self.coder = coders.StrUtf8Coder()\n",
    "\n",
    "    def read_records(self, file_name, range_tracker):\n",
    "        start_offset = range_tracker.start_position()\n",
    "        with self.open_file(file_name) as file_to_read:\n",
    "            file_to_read.seek(start_offset)\n",
    "            read_buffer = file_to_read.read()\n",
    "            data = ''.join(\n",
    "                c for c in read_buffer if c in string.printable or c == '\\n')\n",
    "            data = self.coder.decode(data)\n",
    "            data = _GutenbergSource.clean_text(data)\n",
    "            for text in self.chunks(data):\n",
    "                yield text\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_text(text):\n",
    "        # Replace carraige return and tab characters.\n",
    "        text = text.replace(u'\\r\\n', u'\\n').replace(u'\\r', u'\\n')\n",
    "        text = text.replace(u'\\t', u' ')\n",
    "        # Standardize quotes/apostrophes.\n",
    "        text = re.sub(ur\"[’‘]\",  # noqa: E999\n",
    "                      u\"'\",\n",
    "                      re.sub(ur\"[ ]*`\", u\"'\", text))\n",
    "        # Remove play character prompts like 'WALTER: ...'\n",
    "        text = re.sub(ur\"^[A-Z0-9‘’'., ]+(?:\\([^)]*\\))?[:.][ ]*\",  # noqa\n",
    "                      u'',\n",
    "                      text,\n",
    "                      flags=re.M)\n",
    "        # Remove titles which are lines with all-caps.\n",
    "        text = re.sub(ur'^[A-Z0-9.,-?$#!;:\\'’\"_ ]+$', u'', text, flags=re.M)\n",
    "        # Remove divider lines like '* * * * *' and antiquated punctuation.\n",
    "        text = re.sub(\n",
    "            ur'^[ ]*[-*=_].*[-*=_][ ]*$',\n",
    "            u'',\n",
    "            re.sub(u':--', u':', re.sub('&c', 'etc', text)),\n",
    "            flags=re.M)\n",
    "        # Remove leading spaces.\n",
    "        text = re.sub(u'^[ ]+', u'', text)\n",
    "        # Remove all line breaks except empty lines.\n",
    "        text = re.sub(u'\\n(?=[^\\n])', u' ', text, flags=re.M)\n",
    "        # Remove footnotes and references.\n",
    "        # (they can be nested one deep: [Note [Note in note]])\n",
    "        text = re.sub(  # noqa: W605\n",
    "            ur'\\[[^][]+\\]', u'',\n",
    "            re.sub(ur'\\[[^][]+\\]', '', re.sub(ur'^\\[[^ ]+\\] .*$', u'', text,\n",
    "                                              flags=re.M)))\n",
    "        # Remove underscores\n",
    "        text = text.replace(u'_', u'')\n",
    "        return text\n",
    "\n",
    "    def chunks(self, text):\n",
    "        remainder = text\n",
    "        N = len(remainder)\n",
    "        start = 0\n",
    "        end = 0\n",
    "        while start < N:\n",
    "            end += self.chunk_char_length\n",
    "            while end < N and remainder[end] != u'\\n':\n",
    "                end += 1\n",
    "            # Remove all newlines, double spaces and underscores.\n",
    "            yield re.sub(u' [ ]+',\n",
    "                         u' ',\n",
    "                         re.sub(u'\\n',\n",
    "                                u' ',\n",
    "                                remainder[start:end]))\n",
    "            start = end\n",
    "\n",
    "\n",
    "class ReadGutenbergText(PTransform):\n",
    "    def __init__(self,\n",
    "                 file_pattern=None,\n",
    "                 **kwargs):\n",
    "        super(ReadGutenbergText, self).__init__(**kwargs)\n",
    "        self._source = _GutenbergSource(file_pattern)\n",
    "\n",
    "    def expand(self, pvalue):\n",
    "        return pvalue.pipeline | Read(self._source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A certain king had a beautiful garden, and in the garden stood a tree which bore golden apples. These apples were always counted, and about the time when they began to grow ripe it was found that every night one of them was gone. The king became very angry at this, and ordered the gardener to keep watch all night under the tree. The gardener set his eldest son to watch; but about twelve o'clock he fell asleep, and in the morning another of the apples was missing. Then the second son was ordered to watch; and at midnight he too fell asleep, and in the morning another apple was gone. Then the third son offered to keep watch; but the gardener at first would not let him, for fear some harm should come to him: however, at last he consented, and the young man laid himself under the tree to watch. As the clock struck twelve he heard a rustling noise in the air, and a bird came flying that was of pure gold; and as it was snapping at one of the apples with its beak, the gardener's son jumped up and shot an arrow at it. But the arrow did the bird no harm; only it dropped a golden feather from its tail, and then flew away. The golden feather was brought to the king in the morning, and all the council was called together. Everyone agreed that it was worth more than all the wealth of the kingdom: but the king said, 'One feather is of no use to me, I must have the whole bird.'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('text-small.txt') as f:\n",
    "    s = f.read()\n",
    "    #t = ''.join(c for c in s if c in string.printable or s == '\\n')\n",
    "    #u = coders.StrUtf8Coder().decode(t)\n",
    "    u = s.decode('utf-8-sig')\n",
    "    print(_GutenbergSource.clean_text(u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'my-project'\n",
    "BUCKET = 'gs://my-bucket'\n",
    "\n",
    "def run(argv=None):\n",
    "    \"\"\"Main entry point.\"\"\"\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--input',\n",
    "        dest='input',\n",
    "        default=BUCKET + '/gutenberg/small/*.txt',\n",
    "        help='Input file to process.')\n",
    "    parser.add_argument(\n",
    "        '--output',\n",
    "        dest='output',\n",
    "        default=BUCKET + '/past2present/sentences',\n",
    "        help='Output file to write results to.')\n",
    "    known_args, pipeline_args = parser.parse_known_args(argv)\n",
    "\n",
    "    pipeline_args.extend([\n",
    "        '--runner=DirectRunner',\n",
    "        '--project=' + PROJECT,\n",
    "        '--staging_location=' + BUCKET + '/past2present/staging',\n",
    "        '--temp_location=' + BUCKET + '/tmp',\n",
    "        '--job_name=past2present',\n",
    "    ])\n",
    "    pipeline_options = PipelineOptions(pipeline_args)\n",
    "    # From https://github.com/apache/beam/blob/master/sdks/python/apache_beam\n",
    "    #             /examples/complete/top_wikipedia_sessions.py :\n",
    "    # We use the save_main_session option because one or more DoFn's in this\n",
    "    # workflow rely on global context (e.g., a module imported at module level).\n",
    "    pipeline_options.view_as(SetupOptions).save_main_session = True\n",
    "\n",
    "    def transform(sent):\n",
    "        return (sent.text, Sentence(sent).transform_present())\n",
    "\n",
    "    def format_result(before_after):\n",
    "        (before, after) = before_after\n",
    "        return u'< %s\\n> %s' % (before, after)\n",
    "\n",
    "    with beam.Pipeline(options=pipeline_options) as p:\n",
    "        # Read the text file[pattern] into a PCollection.\n",
    "        chunks = p | ReadGutenbergText(known_args.input)\n",
    "\n",
    "        output = (\n",
    "                chunks\n",
    "                | 'Parse' >> beam.ParDo(ParseWithSpacy())\n",
    "                | 'Split' >> beam.FlatMap(lambda sents: sents)\n",
    "                | 'Transform' >> beam.Map(transform)\n",
    "                | 'Format' >> beam.Map(format_result))\n",
    "\n",
    "        output | WriteToText(known_args.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger().setLevel(logging.INFO)\n",
    "run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
